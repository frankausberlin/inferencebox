networks:
  inferencebox-network:
    driver: bridge

volumes:
  models:
    driver: local
  datasets:
    driver: local
  workspace:
    driver: local
  cache:
    driver: local
  ollama:
    driver: local

services:
  datascience-env:
    build: .
    container_name: datascience-env
    ports:
      - "8888:8888"
    env_file:
      - .env
    environment:
      - JUPYTER_TOKEN=${JUPYTER_TOKEN}
      - JUPYTER_PASSWORD=${JUPYTER_PASSWORD}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - workspace:/home/jupyter/work
      - datasets:/home/jupyter/datasets
      - cache:/home/jupyter/.cache
    networks:
      - inferencebox-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "-H", "Authorization: token ${JUPYTER_TOKEN}", "http://localhost:8888/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11435:11435"
    env_file:
      - .env
    volumes:
      - ollama:/root/.ollama
    networks:
      - inferencebox-network
    environment:
      - OLLAMA_GPU_LAYERS=35
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MAX_QUEUE=512
      - OLLAMA_RUNNERS_DIR=/tmp/runners
      - OLLAMA_TMPDIR=/tmp
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: ["serve"]

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    ports:
      - "3000:8080"
    env_file:
      - .env
    environment:
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_HOST=${WEBUI_HOST}
      - WEBUI_PORT=${WEBUI_PORT}
      - OLLAMA_BASE_URL=http://ollama:11435
      - ENABLE_OLLAMA_API=true
    volumes:
      - ./open-webui:/app/backend/data
    networks:
      - inferencebox-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    command: ["bash", "-c", "cd /app/backend && python3 -c 'from open_webui.main import app; import uvicorn; uvicorn.run(app, host=\"0.0.0.0\", port=8080)'"]