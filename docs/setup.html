<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>setup</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="setup_files/libs/clipboard/clipboard.min.js"></script>
<script src="setup_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="setup_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="setup_files/libs/quarto-html/popper.min.js"></script>
<script src="setup_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="setup_files/libs/quarto-html/anchor.min.js"></script>
<link href="setup_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="setup_files/libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="setup_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="setup_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="setup_files/libs/bootstrap/bootstrap-018f84325ca3ab1b7355738c9941ccce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="llm-setup-ollama-only-configuration" class="level2">
<h2 class="anchored" data-anchor-id="llm-setup-ollama-only-configuration">LLM Setup: Ollama-Only Configuration</h2>
<p><strong>These instructions supersede all previous model configurations and deployment guidelines.</strong></p>
<section id="llm-architecture-overview" class="level3">
<h3 class="anchored" data-anchor-id="llm-architecture-overview">LLM Architecture Overview</h3>
<p>This project uses a <strong>streamlined Ollama-only setup</strong> for reliable local LLM inference:</p>
<ul>
<li><strong>Ollama (Primary)</strong>: Local LLM inference with lightweight models, optimized for local development</li>
<li><strong>Open WebUI</strong>: Web interface for interacting with Ollama models</li>
</ul>
</section>
<section id="key-benefits" class="level3">
<h3 class="anchored" data-anchor-id="key-benefits">Key Benefits</h3>
<ul>
<li><strong>Simplified Architecture</strong>: Single LLM provider reduces complexity</li>
<li><strong>Local-First</strong>: Reliable local inference without cloud dependencies</li>
<li><strong>Cost-Effective</strong>: Uses local resources exclusively</li>
<li><strong>Fast Startup</strong>: No complex model loading or GPU optimization required</li>
</ul>
</section>
<section id="ollama-configuration-primary-local" class="level3">
<h3 class="anchored" data-anchor-id="ollama-configuration-primary-local">Ollama Configuration (Primary, Local)</h3>
<p><strong>Recommended for local development and lightweight inference.</strong></p>
<section id="supported-models" class="level4">
<h4 class="anchored" data-anchor-id="supported-models">Supported Models</h4>
<ul>
<li><strong>Llama 3.2</strong>: 1B, 3B parameters (excellent for local use)</li>
<li><strong>Qwen2.5</strong>: 0.5B, 1.5B, 3B, 7B parameters</li>
<li><strong>Mistral</strong>: 7B parameter models</li>
<li><strong>Phi-3</strong>: 3.8B parameter model</li>
</ul>
</section>
<section id="setup-steps" class="level4">
<h4 class="anchored" data-anchor-id="setup-steps">Setup Steps</h4>
<ol type="1">
<li>Ollama is automatically configured and started</li>
<li>Models are downloaded on first use via Open WebUI</li>
<li>No GPU requirements (CPU-only operation supported)</li>
<li>Persistent storage in Docker volume</li>
</ol>
</section>
</section>
<section id="model-management" class="level3">
<h3 class="anchored" data-anchor-id="model-management">Model Management</h3>
<section id="pulling-new-models" class="level4">
<h4 class="anchored" data-anchor-id="pulling-new-models">Pulling New Models</h4>
<p>To add new models to your Ollama setup, you have several options:</p>
<p><strong>Option 1: Via Open WebUI (Recommended)</strong> 1. Access Open WebUI at <code>http://localhost:3000</code> 2. Click on the model selector in the chat interface 3. Search for available models in the Ollama library 4. Click “Download” next to the desired model 5. Wait for the download to complete (may take several minutes depending on model size and internet speed)</p>
<p><strong>Option 2: Via Docker Command Line</strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pull a specific model (replace 'model-name' with actual model name)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama pull llama3.2:3b</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># List available models to pull</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama pull <span class="at">--help</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Option 3: Via Ollama CLI (if accessing container directly)</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access the Ollama container</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama bash</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Pull a model</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull llama3.2:1b</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exit container</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">exit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="popular-models-to-consider" class="level4">
<h4 class="anchored" data-anchor-id="popular-models-to-consider">Popular Models to Consider</h4>
<ul>
<li><code>llama3.2:1b</code> - Fast, lightweight (1B parameters)</li>
<li><code>llama3.2:3b</code> - Balanced performance (3B parameters)</li>
<li><code>qwen2.5:3b</code> - Good multilingual support</li>
<li><code>mistral:7b</code> - High quality responses (7B parameters)</li>
<li><code>phi3:3.8b</code> - Efficient Microsoft model</li>
</ul>
</section>
<section id="model-switching-in-open-webui" class="level4">
<h4 class="anchored" data-anchor-id="model-switching-in-open-webui">Model Switching in Open WebUI</h4>
<p><strong>Method 1: During Chat Creation</strong> 1. Open Open WebUI at <code>http://localhost:3000</code> 2. Click the “+” button to create a new chat 3. In the model selector dropdown, choose your desired model 4. If the model isn’t downloaded yet, you’ll be prompted to download it 5. Start chatting with the selected model</p>
<p><strong>Method 2: Switching Models in Existing Chat</strong> 1. In an active chat, click on the model name/tag at the top of the chat 2. Select a different model from the dropdown 3. The chat will continue with the new model (conversation history is preserved)</p>
<p><strong>Method 3: Default Model Settings</strong> 1. Click on your profile icon (top right) 2. Go to “Settings” &gt; “Models” 3. Set a default model for new chats 4. This model will be pre-selected when creating new conversations</p>
</section>
<section id="advanced-model-management" class="level4">
<h4 class="anchored" data-anchor-id="advanced-model-management">Advanced Model Management</h4>
<p><strong>Checking Downloaded Models</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># List all downloaded models</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama list</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Check model details</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama show llama3.2:3b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Removing Unused Models</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove a specific model</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama rm llama3.2:1b</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># List models before removing to avoid mistakes</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Model Performance Tuning</strong> - Models run with GPU acceleration when available (configured in docker-compose.yml) - Adjust <code>OLLAMA_GPU_LAYERS</code> in docker-compose.yml for GPU memory usage - Monitor performance with <code>docker stats</code> and <code>docker-compose logs ollama</code></p>
<p><strong>Troubleshooting Model Issues</strong> - <strong>Model won’t download</strong>: Check internet connection and available disk space - <strong>Model not appearing in WebUI</strong>: Restart Open WebUI container or check Ollama logs - <strong>Slow model switching</strong>: Ensure only one model is loaded at a time (configured in docker-compose.yml) - <strong>Out of memory</strong>: Reduce GPU layers or use smaller models</p>
</section>
</section>
<section id="hardware-requirements" class="level3">
<h3 class="anchored" data-anchor-id="hardware-requirements">Hardware Requirements</h3>
<ul>
<li><strong>Recommended</strong>: Any modern CPU/GPU (GPU optional for acceleration)</li>
<li><strong>Minimum</strong>: 4GB RAM, modern CPU</li>
<li><strong>GPU Support</strong>: Automatic detection and utilization when available</li>
</ul>
</section>
<section id="configuration-steps" class="level3">
<h3 class="anchored" data-anchor-id="configuration-steps">Configuration Steps</h3>
<section id="for-local-development-ollama-only" class="level4">
<h4 class="anchored" data-anchor-id="for-local-development-ollama-only">For Local Development (Ollama Only)</h4>
<ol type="1">
<li><strong>Basic Setup:</strong> <code>bash     ./scripts/configure.sh</code>
<ul>
<li>Ollama is configured automatically</li>
<li>Open WebUI starts independently</li>
<li>No vLLM dependency required</li>
</ul></li>
<li><strong>Access Open WebUI:</strong>
<ul>
<li>URL: <code>http://localhost:3000</code></li>
<li>Select models from Ollama library</li>
<li>Models download automatically on first use</li>
</ul></li>
</ol>
</section>
<section id="for-vast.ai-deployment-ollama-only" class="level4">
<h4 class="anchored" data-anchor-id="for-vast.ai-deployment-ollama-only">For Vast.ai Deployment (Ollama Only)</h4>
<ol type="1">
<li><p><strong>Execute Configuration Script:</strong> <code>bash      ./scripts/configure.sh</code> This will automatically:</p>
<ul>
<li>Configure Ollama for optimal performance</li>
<li>Set up GPU acceleration if available</li>
<li>Configure Open WebUI to connect to Ollama</li>
</ul></li>
<li><p><strong>Verify Configuration:</strong> Check your <code>.env</code> file for the following settings: <code>OLLAMA_HOST=0.0.0.0:11435       OPENAI_API_BASE_URL=http://ollama:11435/v1</code></p></li>
<li><p><strong>Interactive Welcome Script:</strong> After deployment, run the welcome script for system information: <code>bash       docker-compose exec datascience-env bash /usr/local/bin/welcome.sh</code> Features:</p>
<ul>
<li>Displays GPU status and VRAM usage</li>
<li>Shows service status (Ollama, Open WebUI, Jupyter)</li>
<li>Lists available scripts and usage hints</li>
<li>Provides Ollama port conflict resolution information</li>
<li>Uses port 11435 (not default 11434) to avoid local Ollama conflicts</li>
</ul></li>
</ol>
</section>
</section>
<section id="deployment-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="deployment-scenarios">Deployment Scenarios</h3>
<section id="local-development-recommended-default" class="level4">
<h4 class="anchored" data-anchor-id="local-development-recommended-default">Local Development (Recommended Default)</h4>
<ul>
<li><strong>LLM</strong>: Ollama only</li>
<li><strong>Startup</strong>: Fast, reliable</li>
<li><strong>Models</strong>: Lightweight, local inference</li>
<li><strong>Cost</strong>: Free, uses local resources</li>
<li><strong>Use Case</strong>: Development, testing, lightweight tasks</li>
</ul>
</section>
<section id="vast.ai-deployment-ollama-only" class="level4">
<h4 class="anchored" data-anchor-id="vast.ai-deployment-ollama-only">Vast.ai Deployment (Ollama Only)</h4>
<p>For vast.ai instances, the configuration script automatically applies: - <strong>GPU Acceleration</strong>: Automatic GPU detection and utilization - <strong>Memory Optimization</strong>: Efficient resource usage for cloud instances - <strong>Model Selection</strong>: Lightweight models optimized for cloud environments - <strong>Single LLM</strong>: Streamlined Ollama-only operation</p>
</section>
</section>
<section id="troubleshooting" class="level3">
<h3 class="anchored" data-anchor-id="troubleshooting">Troubleshooting</h3>
<section id="ollama-issues" class="level4">
<h4 class="anchored" data-anchor-id="ollama-issues">Ollama Issues</h4>
<ul>
<li><strong>Model Download Fails</strong>: Check internet connectivity and Ollama model availability</li>
<li><strong>Slow Performance</strong>: Models may be running on CPU; ensure GPU layers are configured</li>
<li><strong>Port Conflicts</strong>: Verify port 11435 is available (uses non-default port to avoid local Ollama conflicts)</li>
<li><strong>Local Ollama Conflict</strong>: If running Ollama locally, use different ports (11435 for container, 11434 for local)</li>
</ul>
</section>
<section id="open-webui-issues" class="level4">
<h4 class="anchored" data-anchor-id="open-webui-issues">Open WebUI Issues</h4>
<ul>
<li><strong>Won’t Start</strong>: Check that Ollama is healthy (no vLLM dependency)</li>
<li><strong>Model Not Available</strong>: Pull models manually via <code>docker-compose exec ollama ollama pull &lt;model&gt;</code></li>
<li><strong>Connection Issues</strong>: Verify network connectivity between containers</li>
</ul>
</section>
</section>
<section id="performance-tuning" class="level3">
<h3 class="anchored" data-anchor-id="performance-tuning">Performance Tuning</h3>
<section id="ollama-monitoring" class="level4">
<h4 class="anchored" data-anchor-id="ollama-monitoring">Ollama Monitoring</h4>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check Ollama status</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> exec ollama ollama list</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Monitor resource usage</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> stats</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View Ollama logs</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> logs ollama</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="open-webui-monitoring" class="level4">
<h4 class="anchored" data-anchor-id="open-webui-monitoring">Open WebUI Monitoring</h4>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># WebUI logs</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> logs open-webui</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Health check</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> http://localhost:3000/health</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="service-architecture" class="level3">
<h3 class="anchored" data-anchor-id="service-architecture">Service Architecture</h3>
<p>The streamlined setup ensures reliable operation:</p>
<ul>
<li><strong>Ollama Primary</strong>: Core LLM inference service</li>
<li><strong>Open WebUI</strong>: Web interface for model interaction</li>
<li><strong>Independent Operation</strong>: All services start reliably together</li>
<li><strong>Local Focus</strong>: Optimized for local development workflows</li>
</ul>
</section>
<section id="migration-notes" class="level3">
<h3 class="anchored" data-anchor-id="migration-notes">Migration Notes</h3>
<p>If upgrading from previous versions: 1. vLLM service has been removed for simplicity 2. Configuration now focuses exclusively on Ollama 3. Enhanced reliability with single LLM provider 4. Faster startup and reduced resource requirements</p>
<hr>
<p><strong>Note</strong>: These instructions supersede all previous documentation. The Ollama-only setup provides reliable local LLM inference with simplified architecture.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>